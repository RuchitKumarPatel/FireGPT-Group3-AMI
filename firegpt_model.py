# -*- coding: utf-8 -*-
"""FireGPT_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17jbKAnOtJDdtk0er0VZmKUj7KborhwWa
"""

import os
import json
import time
import pickle
import numpy as np
from typing import List, Dict, Optional, Any
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Document processing
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# Vector database
import chromadb

# Embeddings and LLM
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch

# UI
import gradio as gr

print("‚úÖ Libraries imported successfully!")

# Updated FireGPT RAG System Class
class FireGPTRAGSystem:
    """
    Complete RAG system optimized for firefighting knowledge retrieval (Fixed Version)
    """

    def __init__(self, storage_path='/content/drive/MyDrive/FireGPT'):
        self.storage_path = storage_path
        self.embedding_model = None
        self.vector_store = None
        self.llm = None
        self.qa_chain = None
        self.documents = []

    def setup_embedding_model(self, model_name='all-MiniLM-L6-v2'):
        """Setup sentence transformer embedding model"""
        print(f"üîÑ Loading embedding model: {model_name}")

        try:
            self.embedding_model = SentenceTransformer(model_name)
            print(f"‚úÖ Embedding model loaded successfully")
        except Exception as e:
            print(f"‚ùå Error loading embedding model: {e}")
            print("üîÑ Trying fallback model...")
            self.embedding_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')
            print(f"‚úÖ Fallback embedding model loaded")

    def setup_vector_store(self, documents: List[Document]):
        """Setup ChromaDB vector store with updated API"""
        print("üîÑ Setting up ChromaDB vector store...")

        try:
            # Create persistent directory
            persist_directory = os.path.join(self.storage_path, "chroma_db")
            os.makedirs(persist_directory, exist_ok=True)

            # Initialize ChromaDB with new API
            client = chromadb.PersistentClient(path=persist_directory)

            # Create or get collection
            collection_name = "firefighting_knowledge"

            # Delete existing collection if it exists
            try:
                existing_collections = client.list_collections()
                for collection in existing_collections:
                    if collection.name == collection_name:
                        client.delete_collection(collection_name)
                        print("üóëÔ∏è Deleted existing collection")
                        break
            except Exception as e:
                print(f"Note: {e}")

            # Create new collection
            collection = client.create_collection(
                name=collection_name,
                metadata={"description": "FireGPT knowledge base"}
            )

            # Prepare documents and embeddings
            texts = [doc.page_content for doc in documents]
            metadatas = []
            ids = []

            print("üîÑ Generating embeddings...")
            embeddings = self.embedding_model.encode(
                texts,
                show_progress_bar=True,
                convert_to_numpy=True
            )

            # Prepare metadata for ChromaDB (must be strings, ints, floats, or bools)
            for i, doc in enumerate(documents):
                metadata = {}
                for key, value in doc.metadata.items():
                    if isinstance(value, (str, int, float, bool)):
                        # Ensure string values are not too long (ChromaDB limitation)
                        if isinstance(value, str) and len(value) > 500:
                            metadata[key] = value[:500] + "..."
                        else:
                            metadata[key] = value
                    elif value is None:
                        metadata[key] = "null"
                    else:
                        metadata[key] = str(value)

                metadatas.append(metadata)
                ids.append(f"doc_{i}")

            # Add to collection in batches
            batch_size = 50  # Smaller batches for stability
            for i in range(0, len(texts), batch_size):
                end_idx = min(i + batch_size, len(texts))

                batch_embeddings = embeddings[i:end_idx].tolist()
                batch_texts = texts[i:end_idx]
                batch_metadatas = metadatas[i:end_idx]
                batch_ids = ids[i:end_idx]

                collection.add(
                    embeddings=batch_embeddings,
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )

                print(f"‚úÖ Added batch {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size}")

            self.vector_store = collection
            self.documents = documents

            print(f"‚úÖ Vector store created with {len(documents)} documents")

        except Exception as e:
            print(f"‚ùå Error setting up persistent vector store: {e}")
            print("üîÑ Trying in-memory fallback...")
            self._setup_fallback_vector_store(documents)

    def _setup_fallback_vector_store(self, documents: List[Document]):
        """Fallback vector store using in-memory ChromaDB"""
        try:
            # Use in-memory client as fallback
            client = chromadb.Client()

            collection_name = "firefighting_knowledge_fallback"
            collection = client.create_collection(
                name=collection_name,
                metadata={"description": "FireGPT knowledge base (in-memory)"}
            )

            # Limit documents for fallback (prevent memory issues)
            texts = [doc.page_content for doc in documents[:100]]

            print("üîÑ Generating embeddings for fallback...")
            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)

            # Simple metadata
            metadatas = [{"source": f"doc_{i}", "type": "firefighting"} for i in range(len(texts))]
            ids = [f"fallback_doc_{i}" for i in range(len(texts))]

            collection.add(
                embeddings=embeddings.tolist(),
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )

            self.vector_store = collection
            self.documents = documents[:100]

            print(f"‚úÖ Fallback vector store created with {len(texts)} documents")

        except Exception as e:
            print(f"‚ùå Fallback also failed: {e}")
            raise

    def setup_llm(self):
        """Setup language model with simple fallback"""
        print("üîÑ Setting up language model...")

        try:
            # Check if CUDA is available
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"Using device: {device}")

            # Try to load a simple model
            self.llm = pipeline(
                "text-generation",
                model="distilgpt2",  # Lighter model for Colab
                device=0 if device == "cuda" else -1,
                max_length=200,
                do_sample=True,
                temperature=0.7,
                pad_token_id=50256
            )

            print(f"‚úÖ Language model loaded successfully")

        except Exception as e:
            print(f"‚ùå Error loading language model: {e}")
            print("üîÑ Using rule-based fallback...")
            self.llm = "fallback"

    def create_retriever(self, k=5):
        """Create retriever from vector store"""
        class FireGPTRetriever:
            def __init__(self, collection, embedding_model, documents, k=5):
                self.collection = collection
                self.embedding_model = embedding_model
                self.documents = documents
                self.k = k

            def get_relevant_documents(self, query: str):
                try:
                    # Generate query embedding
                    query_embedding = self.embedding_model.encode([query])

                    # Search vector store
                    results = self.collection.query(
                        query_embeddings=query_embedding.tolist(),
                        n_results=self.k
                    )

                    # Convert back to Document objects
                    relevant_docs = []
                    if results['documents'] and results['documents'][0]:
                        for i, doc_text in enumerate(results['documents'][0]):
                            metadata = results['metadatas'][0][i] if results['metadatas'][0] else {}

                            doc = Document(
                                page_content=doc_text,
                                metadata=metadata
                            )
                            relevant_docs.append(doc)

                    return relevant_docs

                except Exception as e:
                    print(f"‚ö†Ô∏è Retrieval error: {e}")
                    return []

        return FireGPTRetriever(self.vector_store, self.embedding_model, self.documents, k)

    def setup_qa_chain(self):
        """Setup the complete QA chain"""
        print("üîÑ Setting up QA chain...")

        # Create custom prompt for firefighting context
        prompt_template = """You are FireGPT, an AI assistant for wildland firefighting operations.

Context: {context}

Question: {question}

Provide a helpful response based on the context. Always prioritize safety in firefighting operations.

Answer:"""

        self.prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        # Create retriever
        self.retriever = self.create_retriever(k=5)

        print("‚úÖ QA chain setup complete")

    def query(self, question: str) -> Dict[str, Any]:
        """Process a query and return response with metadata"""
        start_time = time.time()

        try:
            # Retrieve relevant documents
            relevant_docs = self.retriever.get_relevant_documents(question)

            # Prepare context
            context = "\n\n".join([doc.page_content for doc in relevant_docs[:3]])  # Limit context

            # Generate response
            response = self._generate_response(question, context)

            # Calculate response time
            response_time = time.time() - start_time

            return {
                'answer': response,
                'source_documents': relevant_docs,
                'response_time': response_time,
                'context_used': len(context),
                'sources_count': len(relevant_docs)
            }

        except Exception as e:
            return {
                'answer': f"I encountered an error processing your question: {str(e)}",
                'source_documents': [],
                'response_time': time.time() - start_time,
                'context_used': 0,
                'sources_count': 0
            }

    def _generate_response(self, question: str, context: str) -> str:
        """Generate response using available methods"""
        question_lower = question.lower()
        context_lower = context.lower()

        # Safety-related questions
        if any(term in question_lower for term in ['safety', 'lces', 'danger', 'protocol']):
            return self._generate_safety_response(context)

        # Tactical questions
        elif any(term in question_lower for term in ['attack', 'tactics', 'suppress', 'strategy']):
            return self._generate_tactical_response(context)

        # Equipment questions
        elif any(term in question_lower for term in ['equipment', 'aircraft', 'engine', 'helicopter']):
            return self._generate_equipment_response(context)

        # General response
        else:
            return self._generate_general_response(question, context)

    def _generate_safety_response(self, context: str) -> str:
        if 'lces' in context.lower():
            return """üõ°Ô∏è FIREFIGHTER SAFETY - LCES PROTOCOL:

L - LOOKOUTS: Post personnel to observe fire behavior and weather changes
C - COMMUNICATIONS: Maintain constant contact with crew and supervisors
E - ESCAPE ROUTES: Identify multiple routes to safety zones
S - SAFETY ZONES: Establish areas where firefighters can survive

ADDITIONAL SAFETY MEASURES:
- Follow the 10 Standard Firefighting Orders
- Be aware of the 18 Watch Out Situations
- Never compromise safety for fire suppression
- When in doubt, communicate and retreat to safety

Always prioritize crew safety over property protection."""
        else:
            return """üõ°Ô∏è FIREFIGHTER SAFETY PROTOCOLS:

1. Always establish LCES before engaging fire
2. Maintain situational awareness at all times
3. Follow your agency's standard operating procedures
4. Report hazardous conditions immediately
5. Never take unnecessary risks

For specific safety procedures, consult your incident commander and current fire behavior conditions."""

    def _generate_tactical_response(self, context: str) -> str:
        return """‚öîÔ∏è FIREFIGHTING TACTICS:

DIRECT ATTACK:
- Best for smaller fires (under 10 acres)
- Attack fire perimeter directly with water/foam
- Requires favorable weather conditions
- Allows for quick suppression

INDIRECT ATTACK:
- Used for larger, intense fires
- Build containment lines away from fire edge
- Remove fuel between fire and control line
- Often combined with backburning

PARALLEL ATTACK:
- Construct line parallel to fire edge
- Maintain safe distance from active fire
- Connect to natural barriers
- Suitable for fast-moving fires

Choose tactics based on:
- Fire size and behavior
- Weather conditions
- Available resources
- Crew safety considerations"""

    def _generate_equipment_response(self, context: str) -> str:
        return """üöÅ FIREFIGHTING EQUIPMENT & RESOURCES:

ENGINES:
- Type 1-2: Structural protection, high water capacity
- Type 3-4: Wildland fires, moderate capacity
- Type 5-7: Initial attack, interface areas

AIRCRAFT:
- Helicopters: Precision drops, difficult terrain access
- Air Tankers: Large retardant drops for line construction
- Lead Planes: Guide air tankers to targets

HAND CREWS:
- Hotshot Crews: Elite wildland firefighters
- Initial Attack Crews: First response teams
- Engine Crews: Support and suppression

Select resources based on:
- Incident size and complexity
- Terrain and accessibility
- Weather conditions
- Strategic objectives"""

    def _generate_general_response(self, question: str, context: str) -> str:
        if context.strip():
            return f"""Based on firefighting knowledge and procedures:

{context[:400]}...

For specific operational guidance:
- Consult current fire behavior and weather
- Follow incident command structure
- Use agency standard operating procedures
- Prioritize firefighter safety

Always verify information with qualified incident personnel."""
        else:
            return """For firefighting operations, please consult:

- Current fire behavior and weather information
- Your agency's standard operating procedures
- Incident command structure
- Local fire management authorities

Always prioritize safety and follow established protocols.

This system provides general information only - operational decisions should be made by qualified incident commanders."""

# Chat Interface Class
class FireGPTChatInterface:
    """
    Interactive chat interface for FireGPT using Gradio
    """

    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.conversation_history = []

    def process_message(self, message: str, history: List[List[str]]) -> tuple:
        """Process user message and return response"""
        if not message.strip():
            return history, ""

        # Get response from RAG system
        result = self.rag_system.query(message)
        response = result['answer']

        # Add to conversation history
        history.append([message, response])

        return history, ""

    def create_interface(self):
        """Create Gradio interface"""

        with gr.Blocks(title="FireGPT - Wildland Fire Assistant") as interface:

            # Header
            gr.Markdown("""
            # üî• FireGPT - Wildland Fire Assistant

            Your AI-powered assistant for wildland firefighting operations, safety protocols, and tactical planning.

            **‚ö†Ô∏è SAFETY NOTICE**: This system provides information for educational and planning purposes.
            Always follow your agency's protocols and consult with qualified incident commanders.
            """)

            # Main chat interface
            with gr.Row():
                with gr.Column(scale=4):
                    chatbot = gr.Chatbot(
                        label="FireGPT Assistant",
                        height=400,
                        show_label=True
                    )

                    msg = gr.Textbox(
                        label="Ask FireGPT",
                        placeholder="Ask about fire tactics, safety protocols, equipment...",
                        lines=2
                    )

                    with gr.Row():
                        send_btn = gr.Button("Send", variant="primary")
                        clear_btn = gr.Button("Clear")

                with gr.Column(scale=1):
                    gr.Markdown("""
                    ### üö® Quick Reference

                    **LCES Safety**:
                    - Lookouts
                    - Communications
                    - Escape Routes
                    - Safety Zones

                    **Example Questions**:
                    - "What is LCES protocol?"
                    - "Direct vs indirect attack?"
                    - "Aircraft types in firefighting?"
                    """)

            # Event handlers
            def handle_message(message, history):
                return self.process_message(message, history)

            # Wire up events
            msg.submit(handle_message, [msg, chatbot], [chatbot, msg])
            send_btn.click(handle_message, [msg, chatbot], [chatbot, msg])
            clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])

        return interface

# Initialize the complete system
print("üöÄ Initializing Complete FireGPT System...")

try:
    # Step 1: Initialize RAG system
    rag_system = FireGPTRAGSystem()

    # Step 2: Setup embedding model
    rag_system.setup_embedding_model()

    # Step 3: Load processed documents
    processed_file = '/content/drive/MyDrive/FireGPT/processed/processed_documents.pkl'

    if os.path.exists(processed_file):
        print("üìö Loading processed documents...")
        with open(processed_file, 'rb') as f:
            documents = pickle.load(f)
        print(f"‚úÖ Loaded {len(documents)} processed documents")
    else:
        print("‚ö†Ô∏è Processed documents not found. Creating basic documents...")
        # Create some basic documents if processing failed
        basic_docs = [
            Document(
                page_content="""FIREFIGHTER SAFETY PROTOCOLS - LCES

L - LOOKOUTS: Personnel assigned to observe fire behavior and weather
C - COMMUNICATIONS: Maintain contact with crew, supervisors, and adjacent forces
E - ESCAPE ROUTES: Predetermined routes to safety zones
S - SAFETY ZONES: Areas where firefighters can survive if escape routes are compromised

The LCES protocol is fundamental to wildland firefighter safety and must be established before engaging any fire.""",
                metadata={"source": "Safety Manual", "type": "safety"}
            ),
            Document(
                page_content="""WILDLAND FIRE ATTACK METHODS

DIRECT ATTACK: Used on smaller fires under favorable conditions
- Attack fire perimeter directly
- Use water, foam, dirt, or retardant
- Requires safe access to fire edge

INDIRECT ATTACK: Used on larger, more intense fires
- Build containment lines away from fire
- Remove fuel between fire and control line
- Often combined with backburning operations""",
                metadata={"source": "Tactical Manual", "type": "tactics"}
            )
        ]
        documents = basic_docs

    # Step 4: Setup vector store
    rag_system.setup_vector_store(documents)

    # Step 5: Setup LLM
    rag_system.setup_llm()

    # Step 6: Setup QA chain
    rag_system.setup_qa_chain()

    print("‚úÖ RAG System initialization complete!")

    # Step 7: Test the system
    print("\nüß™ Testing system...")
    test_result = rag_system.query("What is LCES safety protocol?")
    print(f"Test query successful: {len(test_result['answer'])} character response")

    # Step 8: Create chat interface
    print("\nüöÄ Creating FireGPT chat interface...")
    chat_interface = FireGPTChatInterface(rag_system)
    interface = chat_interface.create_interface()

    # Step 9: Launch interface
    print("üî• Launching FireGPT...")
    interface.launch(
        share=True,
        debug=True,
        show_error=True
    )

    print("‚úÖ FireGPT is now running!")

except Exception as e:
    print(f"‚ùå Error during initialization: {e}")
    print("Please check the error and try again.")